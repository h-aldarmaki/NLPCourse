{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Language Model\n\n\nThis is a very basic implementation of a neural language model trained on the Berkeley Restaurant Project Dataset. The code is adapted from the following two tutorials:\n\n* [Neural Machine Translation](https://github.com/kmsravindra/ML-AI-experiments/blob/master/AI/Neural%20Machine%20Translation/Neural%20machine%20translation%20-%20Encoder-Decoder%20seq2seq%20model.ipynb) \n* [Neural Language Model](https://ethen8181.github.io/machine-learning/keras/rnn_language_model_basic_keras.html) \n\nThis model is designed as follows:\n* **input:** one-hot vectors. No projection layer is used for learning word embeddings. \n    * Note: You can try improving this model using pre-trained word embedding.\n\n* **Recurrence:** We use 1 LSTM layer. LSTM is a special type of recurrent neural unit that is more suitable for long sequences compared to regular RNNs. [Learn More](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n\n* **Output:** Softmax layer with 1815 units (the size of the vocabulary in this set). \n\nWe do not model unknown words here, and we only test it for text generation. You will notice that the output is not great. It is in fact inferior to a bigram language model. Possible ways to improve the model include using pre-trained word embeddings, more training data, and experimenting with different network architectures. Decoding could also be improved by resetting the LSTM state and using random sampling instead (here we just keep the LSTM state as another way to randomize the output). \n\nHere is [another way](https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html) to implement neural text generation using pytorch. \n\n\n\n---\n\n# Running the code\n\nYou can just run the code blocks sequentially. Using GPU runtime is recommended for faster training (Runtime --> Change runtime type --> GPU)\n","metadata":{"id":"2LRJny1EFvOO"}},{"cell_type":"code","source":"# download the dataset\n! git clone https://github.com/wooters/berp-trans.git\n","metadata":{"id":"DCgYFNPykuch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#process the data\nimport re\n\n#Let's first read the file\nfilename ='berp-trans/transcript.txt'\nfile = open(filename, 'rt')\ntext = file.read()\nfile.close()\n\ntext = text.split('\\n')\ni=0\nwhile i < len(text):\n  text[i] = re.sub(r'^\\S+\\s+', '', text[i])\n  text[i] = re.sub(r'[<\\[][^>\\]]+[>\\]]\\s+', '' , text[i])\n  i = i + 1\n\nimport nltk\n\n#create FreqDist object\nunigram_dist = nltk.FreqDist()\nfor sen in text:\n  sen = \"<s> \"+ sen +\" </s>\"\n  unigram_dist.update(sen.split())\n      \nvocab = unigram_dist.keys()\ntrainset = \" \".join(text).split()","metadata":{"id":"Eo692MtGkySz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, LSTM, Dense, Embedding\nfrom tensorflow.keras.utils import to_categorical\n\nV_size = len(vocab) #length of vocab\n\n\nword2index = {}\nindex2word = [] \n\nfor word in vocab:\n    index2word.append(word)\n    word2index[word] = len(word2index)\n\nnum_classes = len(word2index)\n\n\nembedding_size = 50\nlstm_size = 256\nmaxlen = 10 #maximum sentence length\n\nencoder_input = Input(shape=(None,V_size))\n\nencoder_LSTM = LSTM(lstm_size,return_state = True)\nencoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\nencoder_states = [encoder_h, encoder_c]\nencoder_dense = Dense(V_size, activation = 'softmax')\nencoder_out = encoder_dense (encoder_outputs)\nmodel = Model(inputs=[encoder_input],outputs=[encoder_out])\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n\nprint(model.summary())\n\n\n# create semi-overlapping sequences of words with\n# a fixed length specified by the maxlen parameter\n\nX = []\ny = []\nN=0\nstep=3\nfor i in range(0, len(trainset) - maxlen, step):\n  N=N+1\n\nX = np.zeros(shape = (N,maxlen, V_size), dtype='float32')\nY = np.zeros(shape = (N,V_size), dtype='float32')\nii=0\nfor i in range(0, len(trainset) - maxlen, step):\n    sent = trainset[i:i + maxlen]\n    next_word = trainset[i + maxlen]\n    for k,w in enumerate(sent):\n        X[ii,k, word2index[w]] = 1\n    Y[ii,word2index[next_word]] = 1\n    ii=ii+1\n\n\nprint('sequence dimension: ', X.shape)\nprint('target dimension: ', Y.shape)\nprint('example sequence:\\n', X[0])\n\n\n\n","metadata":{"id":"VuDcn_VCde1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model\nmodel.fit(X, Y, batch_size = 64, \n                            epochs = 50, \n                            validation_split = 0.2)\n\n\n\n\n","metadata":{"id":"fn1yIY_o1Wpq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoder inference model\ninitial_model = Model(encoder_input, encoder_states)\ntarget_seq = np.zeros((1, 1, V_size))\n\n#Initial token\ntarget_seq[0, 0, word2index['<s>']] = 1\n        \nstates_val = initial_model.predict(target_seq)\n#print(states_val)\n\nencoder_state_input_h = Input(shape=(256,))\nencoder_state_input_c = Input(shape=(256,))\nencoder_input_states = [encoder_state_input_h, encoder_state_input_c]\n\nencoder_out, encoder_h, encoder_c = encoder_LSTM(encoder_input, \n                                                 initial_state=encoder_input_states)\n\nencoder_states = [encoder_h , encoder_c]\n\nencoder_out = encoder_dense(encoder_out)\n\nencoder_model_inf = Model(inputs=[encoder_input] + encoder_input_states,\n                          outputs=[encoder_out] + encoder_states )\n    \n\n","metadata":{"id":"KQgkJE1QAlTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a function for decoding ... \ndef decode_seq(states_val):\n    \n    target_seq = np.zeros((1, 1, V_size))\n    target_seq[0, 0, word2index['<s>']] = 1\n    \n    translated_sent = ''\n    stop_condition = False\n    count =1\n    \n\n    while not stop_condition:\n        \n        decoder_out, decoder_h, decoder_c = encoder_model_inf.predict(x=[target_seq] + states_val)\n        #print(decoder_out)\n        #print(decoder_out.shape)\n        max_val_index = np.argmax(decoder_out[0,:]) #greedy decoding\n        sampled_word = index2word[max_val_index]\n        translated_sent += \" \"+sampled_word\n        count = count+1\n        #stop if </s> is reached or more than 50 words generated ... \n        if ( (sampled_word == '</s>') or (len(translated_sent) > 50)) :\n            stop_condition = True\n        \n        target_seq = np.zeros((1, 1, V_size))\n        target_seq[0, 0,max_val_index]=1\n        states_val = [decoder_h, decoder_c]\n                \n    return translated_sent, states_val\n\nfor i in range(10):\n  sent, states_val = decode_seq(states_val)\n  print(sent)\n\n","metadata":{"id":"DyOulsjlSxiZ"},"execution_count":null,"outputs":[]}]}
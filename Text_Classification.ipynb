{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Classification \n\nIn this project, you will implement a sentiment classifier using different models and compare their performance: Naive Bayes and Logistic Regression. \n\nYou will need the following Python packages:\n* NLTK\n* sklearn\n\n### **The Dataset**\nWe will use the customer reviews dataset, which was first introduced in the following paper:\n\n* \"Mining and summarizing customer reviews\", 2004.\n\nThe dataset contains 2405 positive and 1366 negative reviews. We will use 2000 positive and 1000 negative reviews for training and will leave the remaining reviews for testing. \n\n## Step 1: Download the dataset\n\nRun the following code cell to download the dataset","metadata":{"id":"Hm-joeROL1Mi"}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/h-aldarmaki/NLPCourse/main/data/pos.txt\n!wget https://raw.githubusercontent.com/h-aldarmaki/NLPCourse/main/data/neg.txt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Read and process data\n\n**Exercise:** After uploading the data above, you should have the following files:\n* pos.txt\n* neg.txt\n\nEach file contains customer reviews, each line is a separate review. In the following code block, read the two files, split the lines, and store the resulting lists in two list objects: ```text_pos``` and ```text_neg```\n* **Note**: each should be a list of strings. The input is already tokenized and lower-cased, so there is no need to pre-process the text.  You may optionally stem the words and/or remove punctuation. ","metadata":{"id":"DLQjdV0WNgFK"}},{"cell_type":"code","source":"#your code here\n\n#resulting lists of sentences should be stored in text_pos and text_neg","metadata":{"id":"S_HYQOtXNfEz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Prepare Train/Test sets & labels\n\nIn this step, we will divide the reviews into train and test sets. We will use the first 2000 poitive reviews, and the first 1000 negative reviews for training. The remaining data will be left for testing. \n\nAlso, we will create lists of labels: 1 for positive, and 0 for negative. ","metadata":{"id":"ASyP44IeUntd"}},{"cell_type":"code","source":"train_set = text_pos[0:2000]+text_neg[0:1000]\ntest_set = text_pos[2000:]+text_neg[1000:]\n\n#let's get the number of positive and negative test samples\nlen_pos = len(text_pos[2000:])\nlen_neg = len(text_neg[1000:])\n\n#create a list of labels: 1 repeated 2000 times (for the positive reviews), and 0 repeated 1000 times. \ntrain_y = [1]*2000+[0]*1000\n\n#create a list of labels for the testset, based on the number of positive and negative samples\ntest_y = [1]*len_pos+[0]*len_neg\n","metadata":{"id":"tyOhwaLRV1fc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Feature Extraction\n\n**Exercise**: In this step, you need to extract the Bag-of-words features using the training set. Use the CountVectorizer class as shown in the slides. \n\n","metadata":{"id":"Xry57AWOzSEy"}},{"cell_type":"code","source":"#your code here\n#use CountVectorizer (as shown in slides) to extract BOW features. \n","metadata":{"id":"KPvw3ZLkWRcq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Training\n\n**Exercise:** Train a MultinomialNB (Naive Bayes) classifier - as shown in the slides. The input should be the trasformed training set from the previous step, and the list of labels (```train_y```) created in Step 3. ","metadata":{"id":"Xldg80HRzv5B"}},{"cell_type":"code","source":"#your code here\n#Train a MultinomialNB classifier using your training BOW vectors and labels","metadata":{"id":"sX0OdAUcg9mw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 6: Evaluate the model\n\n**Exercise:** In this step, you need to apply the learned classifier on the test set, and evaluate the performance. The model will give you labels (0 or 1), and the true labels are stored in ```test_y```. Calculate the accuracy and F1 score. ","metadata":{"id":"NHAl5IrU0amw"}},{"cell_type":"code","source":"#Your code here\n\n#apply the classifier on the testset, and calculate accuracy and F1 score. ","metadata":{"id":"TrwC0h760aAX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 7:\n\n**Exercise:*** Repeat steps 5 and 6, but this time use a LogisticRegression classifier (as shown in slides). Calculate the accuracy and F1 score for this classifier","metadata":{"id":"cQiUVyNr03c3"}},{"cell_type":"code","source":"#your code here","metadata":{"id":"yA8GfMV81Fv_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 8:\n\n**Exercise:** Now, using similar steps as what you did above, implement the following:\n* Naive Bayes with tf-idf features\n* Logistic Regression with tf-idf features\n\nReport the performance of both models on the test set, using the accuracy and F1 score. \n\nOut of all the models you implemented, which one has the best performance?","metadata":{"id":"WY02isZD2OK1"}}]}
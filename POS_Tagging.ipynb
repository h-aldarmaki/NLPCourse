{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of POS_Tagging.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUw4Mmej+6whoyBtX5X98k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h-aldarmaki/NLPCourse/blob/main/POS_Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eVJwp32ij19"
      },
      "source": [
        "# **Part-of-Speech Tagging**\n",
        "\n",
        "In this exercise, we will implement part-of-speech tagging using the following models:\n",
        "\n",
        "\n",
        "*   Most Frequent Tag (baseline)\n",
        "*   NLTK pos_tag  function\n",
        "*   Hidden Markov Model\n",
        "\n",
        "The objective of this exercise is to familiarize you with sequence labeling tasks, baselines, and model evaluation. \n",
        "\n",
        "## Data & Tagset\n",
        "\n",
        "We will use the brown corpus. Download the datasets from Blackboard, then upload the files here. \n",
        "\n",
        "The brown tagset includes 87 tags. To simplify things, we will convert the tags to the universal tagset of 17 tags:\n",
        "\n",
        "* ADJ: adjective\n",
        "* ADP: adposition\n",
        "* ADV: adverb\n",
        "* AUX: auxiliary\n",
        "* CCONJ: coordinating conjunction\n",
        "* DET: determiner\n",
        "* INTJ: interjection\n",
        "* NOUN: noun\n",
        "* NUM: numeral\n",
        "* PART: particle\n",
        "* PRON: pronoun\n",
        "* PROPN: proper noun\n",
        "* PUNCT: punctuation\n",
        "* SCONJ: subordinating conjunction\n",
        "* SYM: symbol\n",
        "* VERB: verb\n",
        "* X: other\n",
        "\n",
        "\n",
        "## Tools\n",
        "\n",
        "* We will use ``NLTK.tag.mapping`` for mapping the tags between different tagsets. \n",
        "\n",
        "* We will use ``NLTK.pos_tag``  for tagging and compare the performance with the baseline. \n",
        "\n",
        "* We will use ``nltk.HiddenMarkovModelTagger``  for training a hidden markov model. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DudPSWgi7yG-"
      },
      "source": [
        "# Step 1 :\n",
        "\n",
        "## Step 1.1: Read and process input files\n",
        "\n",
        "After running the following block, you will have two files: ``brown.train.tagged.txt``  and ``brown.test.tagged.txt``\n",
        "\n",
        "The following blocks reads both files and stores the sentences in lists: ``trainset_txt`` and ``testset_txt``"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download files\n",
        "\n",
        "!wget https://raw.githubusercontent.com/h-aldarmaki/NLPCourse/main/data/brown.train.tagged.txt\n",
        "!wget https://raw.githubusercontent.com/h-aldarmaki/NLPCourse/main/data/brown.test.tagged.txt"
      ],
      "metadata": {
        "id": "OjIrHKlSltaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM0Nivs9zhYy"
      },
      "source": [
        "#Let's first read the file: training set\n",
        "filename ='brown.train.tagged.txt'\n",
        "file = open(filename, 'rt')\n",
        "trainset_txt = file.read()\n",
        "file.close()\n",
        "\n",
        "#split sentences by new line character\n",
        "trainset_txt = trainset_txt.split('\\n')\n",
        "\n",
        "#check the output (printing the first 10 sentences)\n",
        "print(trainset_txt[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9sFbp1jz0Qp"
      },
      "source": [
        "#Now let's read the test set file\n",
        "filename ='brown.test.tagged.txt'\n",
        "file = open(filename, 'rt')\n",
        "testset_txt = file.read()\n",
        "file.close()\n",
        "\n",
        "#split sentences\n",
        "testset_txt = testset_txt.split('\\n')\n",
        "\n",
        "print(testset_txt[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUkYlja7HHew"
      },
      "source": [
        "## Step 1.2 \n",
        "\n",
        "In the following block, we separate the words and the tags and store them as a list of tuples. The resulting sentences will be stored in ``trainset`` and ``testset``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usyVgAFgz7-G"
      },
      "source": [
        "#Now we will convert training sentences to tuples in the form (WORD, TAG)\n",
        "#The tuples will be stored in the list trainset\n",
        "import nltk\n",
        "from  nltk.tag import mapping\n",
        "nltk.download(\"universal_tagset\")\n",
        "\n",
        "trainset = []\n",
        "for sent in trainset_txt:\n",
        "   words = sent.split()\n",
        "   sent_parts = []\n",
        "   for word in words:\n",
        "     parts = word.split(\"/\")\n",
        "     word = parts[0].lower()\n",
        "     tag = parts[1].upper()\n",
        "     tag = mapping.map_tag('en-brown', 'universal', tag) #map from Brown tagset to Universal tagset\n",
        "     sent_parts.append((word, tag))\n",
        "   if len(sent_parts) > 0:\n",
        "     trainset.append(sent_parts)\n",
        "\n",
        "\n",
        "#convert test sentences to tuples (WORD, TAG)\n",
        "#The tuples will be stored in the list testset\n",
        "testset = []\n",
        "testset_text = [] # we will also store a text version of the sentences here (will be needed for the HMM model later)\n",
        "for sent in testset_txt:\n",
        "   words = sent.split()\n",
        "   sent_parts = []\n",
        "   sent_words = []\n",
        "   for word in words:\n",
        "     parts = word.split(\"/\")\n",
        "     if parts[0] != '':\n",
        "       word = parts[0].lower()\n",
        "       tag = parts[1].upper()\n",
        "       tag = mapping.map_tag('en-brown', 'universal', tag)\n",
        "       sent_parts.append((word, tag))\n",
        "       sent_words.append(word)\n",
        "   if len(sent_parts) > 0:\n",
        "     testset.append(sent_parts)\n",
        "     testset_text.append(sent_words)\n",
        "\n",
        "#print one sentence from the list to double-check that everything is good\n",
        "print(testset[0])\n",
        "print(testset_text[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvAkt98J_90p"
      },
      "source": [
        "# Step 2: Baseline\n",
        "\n",
        "We will now implement the Most-Frequent-Tag baseline. \n",
        "\n",
        "## Step 2.1 : \n",
        "In the following block, we first count all the tags to get the most frequent tag overall. Your can use ``nltk.FreqDist()`` or ``collections.Counter`` for that.\n",
        "\n",
        "### **Question 1:**\n",
        "Calculate the most frequent tag in the training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO6H9ZyGvL5b"
      },
      "source": [
        "#write code to find the most frequent tag in trainset\n",
        "#Store the tag as most_frequent_tag\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT6STOg6IFnh"
      },
      "source": [
        "## Step 2.2\n",
        "\n",
        "In the next section, we implement the most-frequent-tag baseline tagger. This tagger needs to count the tags for each word, so we go over the list of tuples, and count the tags for each word. The resulting counts will be stored in the dictionary ``word_tags``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5e0YOGcyZMg"
      },
      "source": [
        "#count the tags for each word. Results will be stored in word_tags\n",
        "\n",
        "word_tags = {}\n",
        "\n",
        "for sent in trainset:\n",
        "  for item in sent:\n",
        "    tag = item[1]\n",
        "    if item[0] in word_tags: # if word is already in dictionary\n",
        "      if tag in word_tags[item[0]]: # if the tag is already added for this word\n",
        "        word_tags[item[0]][tag] = word_tags[item[0]][tag] + 1\n",
        "      else:#if the tag has not been added yet, we need to add it now\n",
        "        word_tags[item[0]][tag] = 1\n",
        "    else:#if the word has not been added to dictionary yet\n",
        "      word_tags[item[0]] = {}\n",
        "      word_tags[item[0]][tag] = 1\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf7A8TxoVlx8"
      },
      "source": [
        "## Step 2.3\n",
        "\n",
        "Next, we use the counts we just calculated to tag the test set by finding the most frequent tag for each word. We will also calculate the accuracy of this tagger by comparing the most frequent tag with the true tag. \n",
        "\n",
        "In the process, we will creeate two lists: ``test_true_tags`` to store the true tags in the test set, and ``pred_tags`` to store the tags predicted by our model. We will use these lists in the following steps to calculate the accuracy and to produce the confusion matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT8DcmGk2F2-"
      },
      "source": [
        "#Based on the above counts, find the most frequent tag for each word in the test set\n",
        "#For unseen words, use the most frequent tag overall. \n",
        "\n",
        "test_true_tags = []\n",
        "pred_tags = []\n",
        "for sent in testset:\n",
        "  i=0\n",
        "  while i < len(sent):\n",
        "    item = sent[i]\n",
        "    true_tag = item[1]\n",
        "    test_true_tags.append(true_tag)\n",
        "    word = item[0]\n",
        "    if word in word_tags:\n",
        "      possible_tags = word_tags[word]\n",
        "      for k in sorted(possible_tags, key=possible_tags.get, reverse=True):\n",
        "            pred_tag = k\n",
        "            break;  \n",
        "    else:#if the word is not in our list, we use the most frequent tag overall \n",
        "      pred_tag = most_freq_tag\n",
        "\n",
        "    pred_tags.append(pred_tag)\n",
        "    i = i + 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Question 2:**\n",
        "* (a) Calculate the accuracy of the tagger above. \n",
        "* (b) Produce a confusion matrix \n",
        "\n",
        "**Hint**\n",
        "You may use functions from ``nltk.metrics`` package, such as ``nltk.metrics.ConfusionMatric``\n",
        "https://www.nltk.org/api/nltk.metrics.confusionmatrix.html "
      ],
      "metadata": {
        "id": "eKhy-yH2pYK7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOcJI4rc3wMl"
      },
      "source": [
        "#Your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vXM6W3UBROG"
      },
      "source": [
        "# Step 3: NLTK Tagger\n",
        "\n",
        "In this section, we will use the built-n NLTK pos tagger ``nltk.pos_tag()`` We will calculate the accuracy of this model. \n",
        "\n",
        "## **Question 3**\n",
        "The first block below shows you how to use the NLTK tagger to tag the first sentence from the testset. Given this, write a loop to tag all the sentences in the test set, and add the tags to ``pred_tags``. After that, calculate the accuracy of this tagger and produce the confusion matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W-xakedwtp7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "test_sentence= testset_text[0]\n",
        "print(test_sentence)\n",
        "\n",
        "model_output = nltk.pos_tag(testset_text[0], tagset='universal')\n",
        "print(model_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZpBDThbjNWt"
      },
      "source": [
        "\n",
        "pred_tags = []\n",
        "\n",
        "#pos_tag expects a list of words, so we will use testset_text (created in step 1)\n",
        "for sent in testset_text:\n",
        "  #your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPkvjcZYBYAj"
      },
      "source": [
        "# Step 4: Hidden Markov Model\n",
        "\n",
        "In this section we will use ``nltk.HiddenMarkovModelTagger`` to train an HMM tagger using our trainset. We will then use the tagger to produce tags for the testset (using ``testset_text`` since the input should be a list of words)\n",
        "\n",
        "Note that the tagger might take a while to tag all sentences in the test set. \n",
        "\n",
        "## **Question 4**\n",
        "Calculate the accuracy of this tagger and produce the confusion matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHIOn2ev_dsn"
      },
      "source": [
        "import nltk\n",
        "\n",
        "#training the tagger:\n",
        "tagger = nltk.HiddenMarkovModelTagger.train(trainset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW88xpssdUTI"
      },
      "source": [
        "#Tag the test set\n",
        "results = tagger.tag_sents(testset_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0G9tYwYeCS4"
      },
      "source": [
        "#Calculate the accuracy and confusion matrix\n",
        "\n",
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
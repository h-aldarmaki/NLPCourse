{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Part-of-Speech Tagging**\n\nIn this exercise, we will implement part-of-speech tagging using the following models:\n\n\n*   Most Frequent Tag (baseline)\n*   NLTK pos_tag  function\n*   Hidden Markov Model\n\nThe objective of this exercise is to familiarize you with sequence labeling tasks, baselines, and model evaluation. \n\n## Data & Tagset\n\nWe will use the brown corpus. Download the datasets from Blackboard, then upload the files here. \n\nThe brown tagset includes 87 tags. To simplify things, we will convert the tags to the universal tagset of 17 tags:\n\n* ADJ: adjective\n* ADP: adposition\n* ADV: adverb\n* AUX: auxiliary\n* CCONJ: coordinating conjunction\n* DET: determiner\n* INTJ: interjection\n* NOUN: noun\n* NUM: numeral\n* PART: particle\n* PRON: pronoun\n* PROPN: proper noun\n* PUNCT: punctuation\n* SCONJ: subordinating conjunction\n* SYM: symbol\n* VERB: verb\n* X: other\n\n\n## Tools\n\n* We will use ``NLTK.tag.mapping`` for mapping the tags between different tagsets. \n\n* We will use ``NLTK.pos_tag``  for tagging and compare the performance with the baseline. \n\n* We will use ``nltk.HiddenMarkovModelTagger``  for training a hidden markov model. \n\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"5eVJwp32ij19"}},{"cell_type":"markdown","source":"# Step 1 :\n\n## Step 1.1: Read and process input files\n\nAfter running the following block, you will have two files: ``brown.train.tagged.txt``  and ``brown.test.tagged.txt``\n\nThe following blocks reads both files and stores the sentences in lists: ``trainset_txt`` and ``testset_txt``","metadata":{"id":"DudPSWgi7yG-"}},{"cell_type":"code","source":"#Download files\n\n!wget https://raw.githubusercontent.com/h-aldarmaki/NLPCourse/main/data/brown.train.tagged.txt\n!wget https://raw.githubusercontent.com/h-aldarmaki/NLPCourse/main/data/brown.test.tagged.txt","metadata":{"id":"OjIrHKlSltaV","execution":{"iopub.status.busy":"2022-03-23T12:09:03.553622Z","iopub.execute_input":"2022-03-23T12:09:03.553981Z","iopub.status.idle":"2022-03-23T12:09:06.750647Z","shell.execute_reply.started":"2022-03-23T12:09:03.553892Z","shell.execute_reply":"2022-03-23T12:09:06.749292Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2022-03-23 12:09:04--  https://raw.githubusercontent.com/h-aldarmaki/NLPCourse/main/data/brown.train.tagged.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8589018 (8.2M) [text/plain]\nSaving to: ‘brown.train.tagged.txt’\n\nbrown.train.tagged. 100%[===================>]   8.19M  --.-KB/s    in 0.05s   \n\n2022-03-23 12:09:05 (160 MB/s) - ‘brown.train.tagged.txt’ saved [8589018/8589018]\n\n--2022-03-23 12:09:06--  https://raw.githubusercontent.com/h-aldarmaki/NLPCourse/main/data/brown.test.tagged.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1263059 (1.2M) [text/plain]\nSaving to: ‘brown.test.tagged.txt’\n\nbrown.test.tagged.t 100%[===================>]   1.20M  --.-KB/s    in 0.03s   \n\n2022-03-23 12:09:06 (46.9 MB/s) - ‘brown.test.tagged.txt’ saved [1263059/1263059]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#Let's first read the file: training set\nfilename ='brown.train.tagged.txt'\nfile = open(filename, 'rt')\ntrainset_txt = file.read()\nfile.close()\n\n#split sentences by new line character\ntrainset_txt = trainset_txt.split('\\n')\n\n#check the output (printing the first 10 sentences)\nprint(trainset_txt[:10])","metadata":{"id":"BM0Nivs9zhYy","execution":{"iopub.status.busy":"2022-03-23T12:09:29.367769Z","iopub.execute_input":"2022-03-23T12:09:29.368040Z","iopub.status.idle":"2022-03-23T12:09:29.406907Z","shell.execute_reply.started":"2022-03-23T12:09:29.368005Z","shell.execute_reply":"2022-03-23T12:09:29.405807Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[\"``/`` He/pps almost/rb brought/vbd it/ppo back/rb all/abn the/at way/nn ''/'' ./. \", 'There/ex is/bez a/at vast/jj difference/nn between/in the/at community/nn of/in reconciliation/nn which/wdt the/at New/jj Testament/nn describes/vbz and/cc the/at community/nn of/in congeniality/nn found/vbn in/in the/at average/jj church/nn building/nn ./. ', 'The/at scene/nn ,/, of/in course/nn ,/, should/md be/be nine/cd miles/nns northwest/nr of/in Centralia/np ,/, Illinois/np ,/, the/at geographical/jj center/nn of/in population/nn according/in to/in the/at census/nn ./. ', 'Leading/vbg his/pp$ pony/nn ,/, he/pps hurried/vbd that/dt way/nn ,/, not/ remounting/vbg till/cs he/pps was/bedz well/rb below/in the/at level/nn of/in the/at surrounding/vbg range/nn ./. ', '--/-- 1.0/cd gram/nn premix/nn per/in head/nn per/in day/nn for/in promoting/vbg growth/nn ,/, feed/nn conversion/nn ,/, and/cc getting/vbg lambs/nns on/in full/jj feed/nn earlier/rbr ./. ', 'His/pp$ wife/nn had/hvd said/vbn to/in him/ppo :/: ``/`` Nellie/np is/bez in/in love/nn with/in Clayton/np Roy/np ./. ', 'he/pps looked/vbd dapper/jj in/in a/at lightweight/jj summer/nn suit/nn ,/, brown/jj silk/nn tie/nn and/cc green-tinted/jj soft/jj collar/nn ./. ', 'I/ppss guess/vb you/ppo know/vb about/in that/dt ./. ', \"``/`` Okay/uh ,/, men/nns ''/'' ,/, he/pps said/vbd ./. \", \"``/`` Reporters/nns ''/'' ?/. ?/. \"]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Now let's read the test set file\nfilename ='brown.test.tagged.txt'\nfile = open(filename, 'rt')\ntestset_txt = file.read()\nfile.close()\n\n#split sentences\ntestset_txt = testset_txt.split('\\n')\n\nprint(testset_txt[:10])","metadata":{"id":"M9sFbp1jz0Qp","execution":{"iopub.status.busy":"2022-03-23T12:09:32.898457Z","iopub.execute_input":"2022-03-23T12:09:32.898759Z","iopub.status.idle":"2022-03-23T12:09:32.909735Z","shell.execute_reply.started":"2022-03-23T12:09:32.898729Z","shell.execute_reply":"2022-03-23T12:09:32.908764Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"['In/in Great/jj Expectations/nns the/at hands/nns become/vb almost/rb an/at obsession/nn ./. ', 'Not/ rival/nn ', '3/cd ./. ', 'For/in the/at central/jj storage/nn ,/, Tri-State/jj buys/vbz one/cd acre/nn ,/, Buries/vbz its/pp$ tanks/nns and/cc simply/rb holds/vbz permanent/jj title/nn to/in that/dt piece/nn ./. ', 'Enough/ap trading/vbg stamps/nns were/bed collected/vbn to/to buy/vb a/at 12-passenger/jj station/nn wagon/nn ./. ', 'Cover/vb the/at whole/jj building/nn ,/, bury/vb us/ppo all/abn ,/, by/in nightfall/nn ./. ', 'Then/rb ,/, with/in disappointment/nn evident/jj upon/in their/pp$ faces/nns ,/, they/ppss moved/vbd to/in the/at work/nn ./. ', \"For/in Christ's/np$ sake/nn !/. !/. \", 'To/to have/hv Christ/np dwelling/vbg through/in faith/nn in/in your/pp$ hearts/nns ./. ', 'Standing/vbg there/rb she/pps saw/vbd Shades/nns of/in Night/nn come/vb through/in the/at trees/nns and/cc stop/vb beside/in the/at lodge/nn ,/, silent/jj ,/, almost/rb imperious/jj ,/, her/pp$ body/nn taut/jj ,/, simply/rb standing/vbg without/in speaking/vbg or/cc moving/vbg while/cs the/at wife/nn of/in Walitzee/np waited/vbd ,/, perhaps/rb denying/vbg the/at dread/nn that/wps moved/vbd in/in her/ppo ./. ']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Step 1.2 \n\nIn the following block, we separate the words and the tags and store them as a list of tuples. The resulting sentences will be stored in ``trainset`` and ``testset``","metadata":{"id":"VUkYlja7HHew"}},{"cell_type":"code","source":"#Now we will convert training sentences to tuples in the form (WORD, TAG)\n#The tuples will be stored in the list trainset\nimport nltk\nfrom  nltk.tag import mapping\nnltk.download(\"universal_tagset\")\n\ntrainset = []\nfor sent in trainset_txt:\n   words = sent.split()\n   sent_parts = []\n   for word in words:\n     parts = word.split(\"/\")\n     word = parts[0].lower()\n     tag = parts[1].upper()\n     tag = mapping.map_tag('en-brown', 'universal', tag) #map from Brown tagset to Universal tagset\n     sent_parts.append((word, tag))\n   if len(sent_parts) > 0:\n     trainset.append(sent_parts)\n\n\n#convert test sentences to tuples (WORD, TAG)\n#The tuples will be stored in the list testset\ntestset = []\ntestset_text = [] # we will also store a text version of the sentences here (will be needed for the HMM model later)\nfor sent in testset_txt:\n   words = sent.split()\n   sent_parts = []\n   sent_words = []\n   for word in words:\n     parts = word.split(\"/\")\n     if parts[0] != '':\n       word = parts[0].lower()\n       tag = parts[1].upper()\n       tag = mapping.map_tag('en-brown', 'universal', tag)\n       sent_parts.append((word, tag))\n       sent_words.append(word)\n   if len(sent_parts) > 0:\n     testset.append(sent_parts)\n     testset_text.append(sent_words)\n\n#print one sentence from the list to double-check that everything is good\nprint(testset[0])\nprint(testset_text[0])","metadata":{"id":"usyVgAFgz7-G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Baseline\n\nWe will now implement the Most-Frequent-Tag baseline. \n\n## Step 2.1 : \nIn the following block, we first count all the tags to get the most frequent tag overall. Your can use ``nltk.FreqDist()`` or ``collections.Counter`` for that.\n\n### **Question 1:**\nCalculate the most frequent tag in the training set. ","metadata":{"id":"UvAkt98J_90p"}},{"cell_type":"code","source":"#write code to find the most frequent tag in trainset\n#Store the tag as most_frequent_tag\n","metadata":{"id":"xO6H9ZyGvL5b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2.2\n\nIn the next section, we implement the most-frequent-tag baseline tagger. This tagger needs to count the tags for each word, so we go over the list of tuples, and count the tags for each word. The resulting counts will be stored in the dictionary ``word_tags``","metadata":{"id":"IT6STOg6IFnh"}},{"cell_type":"code","source":"#count the tags for each word. Results will be stored in word_tags\n\nword_tags = {}\n\nfor sent in trainset:\n  for item in sent:\n    tag = item[1]\n    if item[0] in word_tags: # if word is already in dictionary\n      if tag in word_tags[item[0]]: # if the tag is already added for this word\n        word_tags[item[0]][tag] = word_tags[item[0]][tag] + 1\n      else:#if the tag has not been added yet, we need to add it now\n        word_tags[item[0]][tag] = 1\n    else:#if the word has not been added to dictionary yet\n      word_tags[item[0]] = {}\n      word_tags[item[0]][tag] = 1\n  \n\n","metadata":{"id":"F5e0YOGcyZMg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2.3\n\nNext, we use the counts we just calculated to tag the test set by finding the most frequent tag for each word. We will also calculate the accuracy of this tagger by comparing the most frequent tag with the true tag. \n\nIn the process, we will creeate two lists: ``test_true_tags`` to store the true tags in the test set, and ``pred_tags`` to store the tags predicted by our model. We will use these lists in the following steps to calculate the accuracy and to produce the confusion matrix. ","metadata":{"id":"Nf7A8TxoVlx8"}},{"cell_type":"code","source":"#Based on the above counts, find the most frequent tag for each word in the test set\n#For unseen words, use the most frequent tag overall. \n\ntest_true_tags = []\npred_tags = []\nfor sent in testset:\n  i=0\n  while i < len(sent):\n    item = sent[i]\n    true_tag = item[1]\n    test_true_tags.append(true_tag)\n    word = item[0]\n    if word in word_tags:\n      possible_tags = word_tags[word]\n      for k in sorted(possible_tags, key=possible_tags.get, reverse=True):\n            pred_tag = k\n            break;  \n    else:#if the word is not in our list, we use the most frequent tag overall \n      pred_tag = most_freq_tag\n\n    pred_tags.append(pred_tag)\n    i = i + 1\n\n","metadata":{"id":"BT8DcmGk2F2-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### **Question 2:**\n* (a) Calculate the accuracy of the tagger above. \n* (b) Produce a confusion matrix \n\n**Hint**\nYou may use functions from ``nltk.metrics`` package, such as ``nltk.metrics.ConfusionMatric``\nhttps://www.nltk.org/api/nltk.metrics.confusionmatrix.html ","metadata":{"id":"eKhy-yH2pYK7"}},{"cell_type":"code","source":"#Your code here\n","metadata":{"id":"nOcJI4rc3wMl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: NLTK Tagger\n\nIn this section, we will use the built-n NLTK pos tagger ``nltk.pos_tag()`` We will calculate the accuracy of this model. \n\n## **Question 3**\nThe first block below shows you how to use the NLTK tagger to tag the first sentence from the testset. Given this, write a loop to tag all the sentences in the test set, and add the tags to ``pred_tags``. After that, calculate the accuracy of this tagger and produce the confusion matrix. ","metadata":{"id":"2vXM6W3UBROG"}},{"cell_type":"code","source":"import nltk\nnltk.download('averaged_perceptron_tagger')\n\ntest_sentence= testset_text[0]\nprint(test_sentence)\n\nmodel_output = nltk.pos_tag(testset_text[0], tagset='universal')\nprint(model_output)","metadata":{"id":"8W-xakedwtp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npred_tags = []\n\n#pos_tag expects a list of words, so we will use testset_text (created in step 1)\nfor sent in testset_text:\n  #your code here\n","metadata":{"id":"FZpBDThbjNWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Hidden Markov Model\n\nIn this section we will use ``nltk.HiddenMarkovModelTagger`` to train an HMM tagger using our trainset. We will then use the tagger to produce tags for the testset (using ``testset_text`` since the input should be a list of words)\n\nNote that the tagger might take a while to tag all sentences in the test set. \n\n## **Question 4**\nCalculate the accuracy of this tagger and produce the confusion matrix. ","metadata":{"id":"xPkvjcZYBYAj"}},{"cell_type":"code","source":"import nltk\n\n#training the tagger:\ntagger = nltk.HiddenMarkovModelTagger.train(trainset)\n","metadata":{"id":"CHIOn2ev_dsn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tag the test set\nresults = tagger.tag_sents(testset_text)","metadata":{"id":"VW88xpssdUTI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Calculate the accuracy and confusion matrix\n\n#your code here","metadata":{"id":"X0G9tYwYeCS4"},"execution_count":null,"outputs":[]}]}
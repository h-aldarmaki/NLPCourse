{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feedforward Networks\n\nIn this exercise, you will use word embeddings and simple feedforward neural networks for Text classification. \n\n## **Step 1:**\nDownload the data. This is the same dataset used in Text Classification. \n","metadata":{"id":"5kALYUGgXMTf"}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/h-aldarmaki/NLPCourse/main/data/pos.txt\n!wget https://raw.githubusercontent.com/h-aldarmaki/NLPCourse/main/data/neg.txt\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## **Step 2:**\nRun the following code block to download pre-trained word embeddings. These word embeddings were trained on google news data, and the vector size is 300. The download might take around 15 minutes. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport gensim.downloader as gensim_api\n\n#Download vectors. This might take ~15 minutes or longer\nword_vectors = gensim_api.load(\"word2vec-google-news-300\")","metadata":{"id":"WP2ePMoyY152","outputId":"1ea399c7-ac83-469e-e80a-e40f2aef1779"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 3:**\n\nRun the following code block to read the files and split into train/test. ","metadata":{"id":"B9XaWM64xp3D"}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nimport string\n\nfilename ='pos.txt'\nfile = open(filename, 'rt')\ntext_pos = file.read()\nfile.close()\n\nfilename ='neg.txt'\nfile = open(filename, 'rt')\ntext_neg = file.read()\nfile.close()\n\ntext_pos = text_pos.split('\\n')\ntext_neg = text_neg.split('\\n')\n\ntrain_set = text_pos[0:2000]+text_neg[0:1000]\ntest_set = text_pos[2000:]+text_neg[1000:]\n\n#let's get the number of positive and negative test samples\nlen_pos = len(text_pos[2000:])\nlen_neg = len(text_neg[1000:])\n\n#create a list of labels: 1 repeated 2000 times (for the positive reviews), and 0 repeated 1000 times. \ntrain_y = [1]*2000+[0]*1000\n\n#create a list of labels for the testset, based on the number of positive and negative samples\ntest_y = [1]*len_pos+[0]*len_neg\n\n","metadata":{"id":"x81OONKAc-xd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 4:**\n\nYou already experimented with classification using BOW features. Instead of these features, we will use the word embeddings to calculate document vectors. The document vectors are calculated as the average of the word embeddings. The following code block shows you how to do that: it convert the train set into vectors using the average of the word embeddings. Complete the code to do the same processing for the test set. \n","metadata":{"id":"g447MKxCy2Mx"}},{"cell_type":"code","source":"#extract the vocabulary\nV = word_vectors.wv.vocab\n\n#create average vectors for the training set, \n#and store resulting vectors and their labels in X and Y:\ntrain_X=[]\ntrain_Y=[]\ni=0\nfor sent in train_set:\n    words = [word for word in sent.split() if word in V] #extract words that exist in word2vec vocabulary\n    if len(words) > 0:\n        vector = word_vectors[words[0]]\n        j=1\n        while j < len(words):\n            vector = vector + word_vectors[words[j]]\n            j=j+1\n        vector= vector/len(words) #average\n        train_X.append(vector) \n        train_Y.append(train_y[i])\n    i=i+1\n\n\n#Do the same processing for test set\n\n#your code here\n","metadata":{"id":"sTWF_M-rZMYB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 5**\n\nUse the modified data above (train_X and train_Y) to train a logistic regression calassifier. Report the accuracy on the test set  (test_X, and test_Y). ","metadata":{"id":"zFZyIsd-zjO0"}},{"cell_type":"code","source":"#your code here","metadata":{"id":"fhtLUNT0zi5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 6:**\n\nUse the same train and test data used for step 5, train a feedforward neural network with 1 hidden layer. Use sklearn's ```MLPClassifier``` for this part. \nReport the accuracy on the test set using each of the following hyperparameter settings (so you should report 4 accuracy results): \n\n* hidden layer size: 10, activation: relu\n* hidden layer size: 40, activation: relu\n* hidden layer size: 10, activation: tanh\n* hidden layer size: 40, activation: tanh\n\n\nNote that each time you run an experiment with fixed hyperparameters, you will get a different result. Each neural network is initialized randomly, so the results will be different depending on the initialization. Ideally, you should run each experiment multiple times and report the average. ","metadata":{"id":"CK2PcB6ez6MM"}},{"cell_type":"code","source":"#Your code here. ","metadata":{"id":"sVcZJGLrz2oP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Step 7:**\n\nNow implement the same model as in step 7, but this time use Keras. Use the hyperparameter values that gave you the best accuracy in step 7. Report the accuracy on the test set. \n\nAs explained above, you are likely to get slightly different results each time you run this, so ideally you should report the average accuracy of multiple runs. ","metadata":{"id":"L2nv89680Pg1"}},{"cell_type":"code","source":"#your code here","metadata":{"id":"IEU51Z9Sfj4m"},"execution_count":null,"outputs":[]}]}
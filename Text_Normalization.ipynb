{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Normalization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkMz0TFkMYLqRU4M7RBzPG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h-aldarmaki/NLPCourse/blob/main/Text_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE6XcYLLOkH4"
      },
      "source": [
        "#Assignment 1: Text Normalization\n",
        "\n",
        "In this assignment, you will download a text corpus (Sherlock Holmes), and perform some text normalization steps. You will count the number of types before and after normalization. \n",
        "\n",
        "##**What you need to do:**\n",
        "\n",
        "\n",
        "\n",
        "1.   Read the instructions and comments below, and try to understand what is happening in each step. Run all blocks of code and inspect the output in each step. \n",
        "2.   Answer all questions (there are 5 questions) and submit your answers in blackboard. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 1. Downloading the dataset\n",
        "\n",
        "In the following block, we download the dataset, and do some simple processing using command-line toolds like sed and grep. You can execute commands on the underlying operatin system using an exclamation mark (!). For example, the following command is used to delete empty lines from the file sherlock.txt\n",
        "\n",
        "```! sed -i '/^$/' sherlock.txt```\n",
        "\n",
        "The above is not python code, but a unix command (it's like executing something directly from the command-line terminal)\n",
        "\n",
        "Run the following block and examine the output. The file will be saved as sherlock.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr1lSW8WuHE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d4e41f-7084-4729-8f44-fe4a23e80c53"
      },
      "source": [
        "# first, download the text corpurs\n",
        "\n",
        "! wget https://sherlock-holm.es/stories/plain-text/cnus.txt \n",
        "! mv cnus.txt sherlock.txt\n",
        "\n",
        "# delete empty lines:\n",
        "! sed -i '/^$/d' sherlock.txt\n",
        "\n",
        "# I want to delete the first few lines to remove headers and table of contents\n",
        "! grep -n -m 1 'CHAPTER I' sherlock.txt\n",
        "! sed -i '1,80d' sherlock.txt\n",
        "\n",
        "\n",
        "#display the first 50 lines in the file\n",
        "! head -50 sherlock.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-22 07:17:10--  https://sherlock-holm.es/stories/plain-text/cnus.txt\n",
            "Resolving sherlock-holm.es (sherlock-holm.es)... 49.12.76.210, 2a01:4f8:c17:3ff5::2\n",
            "Connecting to sherlock-holm.es (sherlock-holm.es)|49.12.76.210|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3382026 (3.2M) [text/plain]\n",
            "Saving to: ‘cnus.txt’\n",
            "\n",
            "cnus.txt            100%[===================>]   3.22M  3.05MB/s    in 1.1s    \n",
            "\n",
            "2021-09-22 07:17:12 (3.05 MB/s) - ‘cnus.txt’ saved [3382026/3382026]\n",
            "\n",
            "79:          CHAPTER I\n",
            "     In the year 1878 I took my degree of Doctor of Medicine of the\n",
            "     University of London, and proceeded to Netley to go through the\n",
            "     course prescribed for surgeons in the army. Having completed my\n",
            "     studies there, I was duly attached to the Fifth Northumberland\n",
            "     Fusiliers as Assistant Surgeon. The regiment was stationed in India\n",
            "     at the time, and before I could join it, the second Afghan war had\n",
            "     broken out. On landing at Bombay, I learned that my corps had\n",
            "     advanced through the passes, and was already deep in the enemy's\n",
            "     country. I followed, however, with many other officers who were in\n",
            "     the same situation as myself, and succeeded in reaching Candahar in\n",
            "     safety, where I found my regiment, and at once entered upon my new\n",
            "     duties.\n",
            "     The campaign brought honours and promotion to many, but for me it had\n",
            "     nothing but misfortune and disaster. I was removed from my brigade\n",
            "     and attached to the Berkshires, with whom I served at the fatal\n",
            "     battle of Maiwand. There I was struck on the shoulder by a Jezail\n",
            "     bullet, which shattered the bone and grazed the subclavian artery. I\n",
            "     should have fallen into the hands of the murderous Ghazis had it not\n",
            "     been for the devotion and courage shown by Murray, my orderly, who\n",
            "     threw me across a pack-horse, and succeeded in bringing me safely to\n",
            "     the British lines.\n",
            "     Worn with pain, and weak from the prolonged hardships which I had\n",
            "     undergone, I was removed, with a great train of wounded sufferers, to\n",
            "     the base hospital at Peshawar. Here I rallied, and had already\n",
            "     improved so far as to be able to walk about the wards, and even to\n",
            "     bask a little upon the verandah, when I was struck down by enteric\n",
            "     fever, that curse of our Indian possessions. For months my life was\n",
            "     despaired of, and when at last I came to myself and became\n",
            "     convalescent, I was so weak and emaciated that a medical board\n",
            "     determined that not a day should be lost in sending me back to\n",
            "     England. I was dispatched, accordingly, in the troopship Orontes, and\n",
            "     landed a month later on Portsmouth jetty, with my health\n",
            "     irretrievably ruined, but with permission from a paternal government\n",
            "     to spend the next nine months in attempting to improve it.\n",
            "     I had neither kith nor kin in England, and was therefore as free as\n",
            "     air--or as free as an income of eleven shillings and sixpence a day\n",
            "     will permit a man to be. Under such circumstances, I naturally\n",
            "     gravitated to London, that great cesspool into which all the loungers\n",
            "     and idlers of the Empire are irresistibly drained. There I stayed for\n",
            "     some time at a private hotel in the Strand, leading a comfortless,\n",
            "     meaningless existence, and spending such money as I had, considerably\n",
            "     more freely than I ought. So alarming did the state of my finances\n",
            "     become, that I soon realized that I must either leave the metropolis\n",
            "     and rusticate somewhere in the country, or that I must make a\n",
            "     complete alteration in my style of living. Choosing the latter\n",
            "     alternative, I began by making up my mind to leave the hotel, and to\n",
            "     take up my quarters in some less pretentious and less expensive\n",
            "     domicile.\n",
            "     On the very day that I had come to this conclusion, I was standing at\n",
            "     the Criterion Bar, when some one tapped me on the shoulder, and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoDq-v8eQphf"
      },
      "source": [
        "## 2. Open the files and count word types\n",
        "\n",
        "In the following few blocks, we will read the file 'sherlock.txt' in python, then do some counting. Instead of manually counting word types, we will use the ```Counter``` class from the ```collections``` package. \n",
        "\n",
        "Inspect the code below and run each block, then inspect the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c7EPkuJy7Hb"
      },
      "source": [
        "\n",
        "# let's first read the whole file and store it in the variable 'text'\n",
        "filename ='sherlock.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "\n",
        "# split words using whitespaces ... \n",
        "words = text.split()\n",
        "print(words[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UgmlPYy5lEU"
      },
      "source": [
        "# now let's count the number of word types! \n",
        "# we will use the Counter class from collections\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "counter = Counter(words)\n",
        "print(f\"Number of unique word types: {len(counter)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "354ObzaP6pBG"
      },
      "source": [
        "# if you want to know the frequency of a specific word:\n",
        "n = counter[\"Sherlock\"]\n",
        "print(f\"The word 'Sherlock' occurred {n} times\")\n",
        "n = counter[\"the\"]\n",
        "print(f\"The word 'the' occurred {n} times\")\n",
        "n = counter[\"The\"]\n",
        "print(f\"The word 'The' occurred {n} times\")\n",
        "n = counter[\"doesn't\"]\n",
        "print(f\"The word 'doesn't occurred {n} times\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ec2_7KJRauD"
      },
      "source": [
        "##3. Tokenization\n",
        "\n",
        "We will now use ```nltk``` for word tokenization. We will use the function ```word_tokenize``` which takes a string and returns a list of tokens. Note that some NLTK tools require downloading associated resources, so you will also see calls to ```nltk.download()``` as needed. \n",
        "\n",
        "## **Question 1:**\n",
        "\n",
        "* (a) Before tokenization, how many word tokens did we have in the original text? And how many unique word types did we have?\n",
        "\n",
        "* (b) After tokenization, how many word tokens do we have in the text? And how many unique word types do we now have?\n",
        "\n",
        "* (c) What is the most frequent word in this corpus (after tokenization)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWUXtZvq7s7Y"
      },
      "source": [
        "# Now let's do some normalizations ... \n",
        "\n",
        "# convert the text to lowercase characters\n",
        "\n",
        "text = text.lower()\n",
        "\n",
        "#import required function and download resources\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# tokenize text \n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens[:100])\n",
        "\n",
        "word_tokenize(\"Mr. Holmes isn't here, unfortunately.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6BAITPlUETc"
      },
      "source": [
        "#Removing Punctuation\n",
        "\n",
        "In some applications, punctuation are not useful and we would want to remove them. In the following block, I show you how to easily remove all punctuation from tokenized lists of words:\n",
        "\n",
        "##**Question 2:**\n",
        "\n",
        "How many unique types do we have after removing all punctuation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHkTZ08j9D1d"
      },
      "source": [
        "#we can remove punctuation marks:\n",
        "import string\n",
        "print(string.punctuation) #this is a list of all punctuation marks\n",
        "\n",
        "# remove all tokens that are not alphanumeric. The resulting list is stored in words\n",
        "words = []\n",
        "for t in tokens:\n",
        "  if t not in string.punctuation:\n",
        "    words.append(t)\n",
        "\n",
        "#alternatively, you could use the following one-liner to do exactly the same thing!\n",
        "#words = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "\n",
        "print(words[:100])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQHl23y-UK1_"
      },
      "source": [
        "##4. Splitting Sentences\n",
        "\n",
        "Without punctuation, now we cannot even tell where sentences start and end. Identifying where sentences start and end is not super easy. In a paragraph, a sentence could end with a period (.), an exclamation mark (!) or a question mark (?). But there are other occurrences of a period that do not indicate the end of sentence, for example Mr. PhD. etc. \n",
        "\n",
        "NLTK has a trained function called ```sent_tokenize``` which can reliably split a text into a list of sentences. We need to split sentences before tokenizing the text and removing punctuation, so we will go back a few steps and  apply this function on the original text and then re-tokenize the text sentence by sentence. \n",
        "\n",
        "\n",
        "##**Question 3:***\n",
        "\n",
        "How many sentneces do we have in this corpus?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl6A3v699oyH"
      },
      "source": [
        "#keep sentences before processing the words ... we will use sentence tokenizer\n",
        "\n",
        "from nltk import sent_tokenize  #this is a pre-trained sentence tokenizer model\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(sentences[0])\n",
        "print(sentences[1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTtPFO1_VG1e"
      },
      "source": [
        "### Tokenizing the sentences\n",
        "\n",
        "Now that we have a list of sentences, we can go over them one by one and apply ```word_tokenize```. In python, this can be done in one line of code as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPaSxNYV-Bh2"
      },
      "source": [
        "sen_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "print(\" \".join(sen_tokens[0]))\n",
        "print(\" \".join(sen_tokens[1]))\n",
        "print(\" \".join(sen_tokens[3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFKEc0A788kI"
      },
      "source": [
        "##Removing Punctuation from the list\n",
        "\n",
        "Now that we have a list of lists, we will need to use a for loop to process each sentence and remove the punctuation marks. We will store the result in ```sen_words```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2v8LjB7-tIp"
      },
      "source": [
        "#remove punctuation\n",
        "sen_words=[]\n",
        "for sentence in sen_tokens:\n",
        "  sen_words.append([word for word in sentence if not word in string.punctuation])\n",
        "\n",
        "print(\" \".join(sen_words[0]))\n",
        "print(\" \".join(sen_words[1]))\n",
        "print(\" \".join(sen_words[3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NythOBMXeFl"
      },
      "source": [
        "##5. Stemming\n",
        "\n",
        "We will now use the Porter Stemmer to normalize the text even more. In practice, we only use stemming is some applications where we don't have enough data and we need to reduce the number of types. Notice that the output now is made up of stems, some of which are not valid English words. \n",
        "\n",
        "To count the number of stems, we cannot use ```Counter``` directly since now we have a list of lists. We will first need to flatten the list (convert it to a one-dimensional list, rather than a list of lists). To do that, we will use the ```flatten``` function from the package ```pandas``` as done in the second code block below. \n",
        "\n",
        "##**Question 4:**\n",
        "\n",
        "How many unique word types do we now have after stemming?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnMYSE5tBbHy"
      },
      "source": [
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "sen_stemmed = []\n",
        "for sentence in sen_words:\n",
        "  sen_stemmed.append([porter.stem(word) for word in sentence])\n",
        "print(\" \".join(sen_stemmed[0]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eZXqddECqBR"
      },
      "source": [
        "from pandas.core.common import flatten\n",
        "\n",
        "counter = Counter(flatten(sen_words))\n",
        "\n",
        "print(f\"Number of unique tokens: {len(counter)}\")\n",
        "\n",
        "n = counter[\"land\"]\n",
        "print(f\"The word 'land' occurred {n} times\")\n",
        "n = counter[\"landing\"]\n",
        "print(f\"The word 'landing' occurred {n} times\")\n",
        "n = counter[\"doesn't\"]\n",
        "print(f\"The word 'doesn't' occurred {n} times\")\n",
        "n = counter[\"n't\"]\n",
        "print(f\"The word 'n't' occurred {n} times\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D0O-dKMC9sf"
      },
      "source": [
        "counter = Counter(flatten(sen_stemmed))\n",
        "\n",
        "print(f\"Number of unique tokens: {len(counter)}\")\n",
        "n = counter[\"land\"]\n",
        "print(f\"The word 'land' occurred {n} times\")\n",
        "n = counter[\"landing\"]\n",
        "print(f\"The word 'landing' occurred {n} times\")\n",
        "n = counter[\"n't\"]\n",
        "print(f\"The word 'n't' occurred {n} times\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A26LayYXY8-m"
      },
      "source": [
        "##6. Lemmatization\n",
        "\n",
        "Stemming is a crude form of lemmatization. Stemming is done using blind rules and it results in non-words. Lemmatization is a more intelligent form of normalization that results in actual 'lemmas' or word roots. However, it requires additional resources (in this case WordNet) and it's also a little slower. \n",
        "\n",
        "In the following block, we use NLTK's ```WordNetLemmatizer``` to lemmatize the text. Note that we need to download wordnet first. \n",
        "\n",
        "NOTE: we are doing lemmatization without specifying part-of-speech (noun, vern, etc.) so the lemmatization may not be accurate. \n",
        "\n",
        "##**Question 5:**\n",
        "\n",
        "* (a) How many unique types do we now have after lemmatization?\n",
        "* (b) What are the 10 most frequent lemmas in the corpus?\n",
        "* (c) The lemmatization in the code below is not accurate because it assumes everything is a noun. What would you need to have better lammatization? What type of additional steps would you need for that?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2vskBek5kJt"
      },
      "source": [
        "#lemmatization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "sen_lemmatized = []\n",
        "for sentence in sen_words:\n",
        "  sen_lemmatized.append([lem.lemmatize(word) for word in sentence])\n",
        "print(\" \".join(sen_lemmatized[0]))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/h-aldarmaki/NLPCourse/blob/main/Text_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Assignment 1: Text Normalization\n\nIn this assignment, you will download a text corpus (Sherlock Holmes), and perform some text normalization steps. You will count the number of types before and after normalization. \n\n## **What you need to do:**\n\n\n\n1.   Read the instructions and comments below, and try to understand what is happening in each step. Run all blocks of code and inspect the output in each step. \n2.   Answer all questions (there are 5 questions). \n\n\n\n\n## 1. Downloading the dataset\n\nIn the following block, we download the dataset, and do some simple processing using command-line tools like sed and grep. You can execute commands on the underlying operatin system using an exclamation mark (!). For example, the following command is used to delete empty lines from the file sherlock.txt\n\n```! sed -i '/^$/' sherlock.txt```\n\nThe above is not python code, but a unix command (it's like executing something directly from the command-line terminal)\n\nRun the following block and examine the output. The file will be saved as sherlock.txt","metadata":{"id":"TE6XcYLLOkH4"}},{"cell_type":"code","source":"# first, download the text corpurs\n\n! wget https://sherlock-holm.es/stories/plain-text/cnus.txt \n! mv cnus.txt sherlock.txt\n\n# delete empty lines:\n! sed -i '/^$/d' sherlock.txt\n\n# I want to delete the first few lines to remove headers and table of contents\n! grep -n -m 1 'CHAPTER I' sherlock.txt\n! sed -i '1,80d' sherlock.txt\n\n\n#display the first 50 lines in the file\n! head -50 sherlock.txt","metadata":{"id":"Tr1lSW8WuHE5","outputId":"19d4e41f-7084-4729-8f44-fe4a23e80c53"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Open the files and count word types\n\nIn the following few blocks, we will read the file 'sherlock.txt' in python, then do some counting. Instead of manually counting word types, we will use the ```Counter``` class from the ```collections``` package. \n\nInspect the code below and run each block, then inspect the output. \n\n## **Question 1:**\n\n* (a) How many times does the word 'Go' occur in this text?\n* (b) How many times does the word 'go' occur in this text?\n* (c) How many times does the word 'doesn't' occur in this text?\n* (d) How many times does the word 'does' occur in this text?\n* (e) What is the most frequent word in this text?\n","metadata":{"id":"aoDq-v8eQphf"}},{"cell_type":"code","source":"\n# let's first read the whole file and store it in the variable 'text'\nfilename ='sherlock.txt'\nfile = open(filename, 'rt')\ntext = file.read()\nfile.close()\n\n# split words using whitespaces ... \nwords = text.split()\nprint(words[:100])","metadata":{"id":"5c7EPkuJy7Hb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now let's count the number of word types! \n# we will use the Counter class from collections\n\nfrom collections import Counter\n\ncounter = Counter(words)\nprint(f\"Number of unique word types: {len(counter)}\")","metadata":{"id":"7UgmlPYy5lEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if you want to know the frequency of a specific word:\nn = counter[\"Sherlock\"]\nprint(f\"The word 'Sherlock' occurred {n} times\")\n\n#find the most frequent words:\nprint(counter.most_common(3))","metadata":{"id":"354ObzaP6pBG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Splitting Sentences\n\nIdentifying where sentences start and end is not super easy. In a paragraph, a sentence could end with a period (.), an exclamation mark (!) or a question mark (?). But there are other occurrences of a period that do not indicate the end of a sentence, for example Mr. PhD. etc. \n\nNLTK has a trained function called ```sent_tokenize``` which can reliably split a text into a list of sentences. We need to split sentences before doing any further processing.  \n\n\n## **Question 1:**\n\nIn the code block below, split ``text`` into sentences, and store the result as ``sentences``. How many sentneces do we have in this corpus?","metadata":{"id":"KQHl23y-UK1_"}},{"cell_type":"code","source":"from nltk import sent_tokenize  #this is a pre-trained sentence tokenizer model\n\n#your code here:\n\n\n\n#Check the output to make sure everything is fine\nprint(sentences[0])\nprint(sentences[1])","metadata":{"id":"fl6A3v699oyH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Tokenization\n\nWe will now use ```nltk``` for word tokenization. We will use the function ```word_tokenize``` which takes a string and returns a list of tokens. Note that some NLTK tools require downloading associated resources, so you will also see calls to ```nltk.download()``` as needed. Note that ``setences`` is a list, so we will need to loop over the sentences and apply the functions to each sentence separately.\n\n## **Question 2:**\n\n* (a) Before tokenization, how many word tokens did we have in the original text? And how many unique word types did we have?\n\n* (b) After tokenization, how many word tokens do we have in the text? And how many unique word types do we now have?\n\n* (c) What is the most frequent word in this corpus (after tokenization)?\n\n### **Hint**\nEach element in ``tok_sentences`` will be a list of tokens. You will need to convert this to a one-dimensional list of tokens before you can apply ``Counter`` correctly. To do that, you can use the ``flatten`` function ","metadata":{"id":"zTtPFO1_VG1e"}},{"cell_type":"code","source":"\n#import required function and download resources\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n\n#the following can be used to convert any list to a one-dimensional list\nfrom pandas.core.common import flatten\n\n\ntok_sentences=[]\nfor sen in sentences:\n    #your code here, follow the steps below:\n    \n    #1. convert sen to lowercase   \n    #2. apply word_tokenize\n    #3  append the result to tok_sentences\n\n\n#Check the output to make sure everything is fine\nprint(\" \".join(tok_sentences[0]))\nprint(\" \".join(tok_sentences[1]))\nprint(\" \".join(tok_sentences[3]))\n\n\nflat_sentence_list=flatten(tok_sentences)\n#Now you can count the word types:\n","metadata":{"id":"WPaSxNYV-Bh2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Removing Punctuation\n\nIn some applications, punctuation are not useful and we would want to remove them. In the following block, I show you how to easily remove all punctuation from tokenized lists of words. \n\n## **Question 3:**\n\nHow many unique types do we have after removing all punctuation?","metadata":{"id":"_6BAITPlUETc"}},{"cell_type":"code","source":"#we can remove punctuation marks:\nimport string\nprint(string.punctuation) #this is a list of all punctuation marks\n\n#remove punctuation\nno_punc_sentences=[]\nfor sentence in tok_sentences:\n    new_sen=[]\n    for word in sentnece:\n        if word not in string.punctuation:\n            new_sen.append(word)\n    no_punc_sentences.append(word)\n\nprint(\" \".join(no_punc_sentences[0]))\nprint(\" \".join(no_punc_sentences[1]))\nprint(\" \".join(no_punc_sentences[3]))\n","metadata":{"id":"pHkTZ08j9D1d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Stemming\n\nWe will now use the Porter Stemmer to normalize the text even more. In practice, we only use stemming is some applications where we don't have enough data and we need to reduce the number of types. Notice that the output now is made up of stems, some of which are not valid English words. \n\n## **Question 4:**\n\nStem all the words in each sentence. How many unique word types do we now have after stemming?","metadata":{"id":"3NythOBMXeFl"}},{"cell_type":"code","source":"\nfrom nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\n\nsen_stemmed = []\nfor sentence in no_punc_sentences:\n  #your code here:\n\n\n#flatten & count the word types","metadata":{"id":"LnMYSE5tBbHy"},"execution_count":null,"outputs":[]}]}
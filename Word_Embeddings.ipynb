{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Word Embeddings\n\nIn this exercise, you will see how to train and use word embeddings using the ``gensim `` package\n\n\n## **Step 1:**\nRun the following code block to download pre-trained word embeddings. These word embeddings were trained on google news data, and the vector size is 300. The download might take around 15 minutes. ","metadata":{"id":"5kALYUGgXMTf"}},{"cell_type":"code","source":"import numpy as np\nimport gensim.downloader as gensim_api\n\n#Download vectors. This might take ~15 minutes or longer\nword_vectors = gensim_api.load(\"word2vec-google-news-300\")","metadata":{"id":"WP2ePMoyY152","outputId":"1ea399c7-ac83-469e-e80a-e40f2aef1779"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO\n#Pearson correlation w. human scores from file\n! wget http://alfonseca.org/pubs/ws353simrel.tar.gz\n! tar -xf ws353simrel.tar.gz\n! head wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt\n\n","metadata":{"id":"u4p5ywb6Qqq6","outputId":"3701d96b-fde0-4b43-f675-d9a8ce3d0f9b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2:\n\nUsing the word embeddings stored in ``word_vectors``, find the most similar words to a given word\n\n### **Question 1:**\n\n* (a) What are the 10 closest words to \"france\" ?\n* (b) What are the 10 closest words to \"great\" ?\n* (c) What are the 10 closest words to \"film\" ?\n","metadata":{}},{"cell_type":"code","source":"#Your code here","metadata":{"id":"rzZZNOn8VDw4","outputId":"519accd1-29aa-45e9-dadc-cd51d67acc66"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3:\n\nAnswer Analogy Questions.\n\n### **Question 2:**\n\nUsing the word embeddings stored in ``word_vectors``, complete the following sentences:\n\n* (a) Man is to uncle is like woman is to ..... \n* (b) work is to working is like talk is to .... \n* (c) France is to paris is like Germany is to .....","metadata":{}},{"cell_type":"code","source":"#your code here","metadata":{"id":"mSOFoWJPlqOi","outputId":"ddc9ceeb-4510-4bcf-98c5-f9ba5e058b06"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4:\n\nNow we will plot the word embeddings to see how they are related. The following block implements a function for computing PCA components, which will project the 300-dimensional vectors to 2 dimensions so we can plot them.The code is taken from: https://necromuralist.github.io/Neurotic-Networking/posts/nlp/pca-dimensionality-reduction-and-word-vectors/","metadata":{}},{"cell_type":"code","source":"import numpy\ndef compute_pca(X: numpy.ndarray, n_components: int=2) -> numpy.ndarray:\n    \"\"\"Calculate the principal components for X\n\n    Args:\n       X: of dimension (m,n) where each row corresponds to a word vector\n       n_components: Number of components you want to keep.\n\n    Return:\n       X_reduced: data transformed in 2 dims/columns + regenerated original data\n    \"\"\"\n    # you need to set axis to 0 or it will calculate the mean of the entire matrix instead of one per row\n    X_demeaned = X - X.mean(axis=0)\n\n    # calculate the covariance matrix\n    # the default numpy.cov assumes the rows are variables, not columns so set rowvar to False\n    covariance_matrix = numpy.cov(X_demeaned, rowvar=False)\n\n    # calculate eigenvectors & eigenvalues of the covariance matrix\n    eigen_vals, eigen_vecs = numpy.linalg.eigh(covariance_matrix)\n\n    # sort eigenvalue in increasing order (get the indices from the sort)\n    idx_sorted = numpy.argsort(eigen_vals)\n\n    # reverse the order so that it's from highest to lowest.\n    idx_sorted_decreasing = list(reversed(idx_sorted))\n\n    # sort the eigen values by idx_sorted_decreasing\n    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]\n\n    # sort eigenvectors using the idx_sorted_decreasing indices\n    # We're only sorting the columns so remember to get all the rows in the slice\n    eigen_vecs_sorted = eigen_vecs[:, idx_sorted_decreasing]\n\n    # select the first n eigenvectors (n is desired dimension\n    # of rescaled data array, or dims_rescaled_data)\n    # once again, make sure to get all the rows and only slice the columns\n    eigen_vecs_subset = eigen_vecs_sorted[:, :n_components]\n\n    # transform the data by multiplying the transpose of the eigenvectors \n    # with the transpose of the de-meaned data\n    # Then take the transpose of that product.\n    X_reduced = numpy.dot(eigen_vecs_subset.T, X_demeaned.T).T\n    return X_reduced\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom sklearn.decomposition import PCA\ndef display_pca_scatterplot(model, words=None, sample=0):\n    if words == None:\n        if sample > 0:\n            words = np.random.choice(list(model.vocab.keys()), sample)\n        else:\n            words = [ word for word in model.vocab ]\n    else:\n            words = [word for  word in words if word in model.vocab] \n    word_vectors = np.array([model[w] for w in words])\n\n    twodim = PCA().fit_transform(word_vectors)[:,:2]\n    \n    plt.figure(figsize=(6,6))\n    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n    for word, (x,y) in zip(words, twodim):\n        plt.text(x+0.05, y+0.05, word)","metadata":{"id":"ZulWcbsXdqHx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot \n\nUsing the above method ``display_pca_scatterplot``, you can now display a plot showing a subset of words of your choice. An example is shown below. Select other words and plot them to see how they are related in the vector space. ","metadata":{}},{"cell_type":"code","source":"display_pca_scatterplot(word_vectors, \n                        ['paris', 'berlin', 'france','germany' ])","metadata":{"id":"mVG1BDhOe5Dg","outputId":"599646b3-7b85-4d5c-f330-54c2fcaf3017"},"execution_count":null,"outputs":[]}]}